{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Topaz pipeline\n",
    "\n",
    "The Topaz particle picking pipeline proceeds as follows:\n",
    "\n",
    "(Before running Topaz, optional): label a small (100-1000, more is likely to give better results) number of particles on your micrographs using the software of your choice. This can be skipped when using the pretrained models bundled with Topaz. \n",
    "\n",
    "1. (Preprocessing) Micrographs are downsampled and normalized, any labeled particle coordinates also need to be scaled appropriately\n",
    "2. (Training, optional) The particle detection model is trained on the preprocessed micrographs using the labeled particle coordinates. This requires setting the expected number of particles per micrograph. This can be skipped when using the pretrained models bundled with Topaz.\n",
    "3. (Extraction) Using a trained model, particle coordinates and their associated scores are extracted from the micrographs. This requires knowing the particle radius in pixels on the downsampled micrographs.\n",
    "\n",
    "(Optional postprocessing): examine classifier performance, rescale particle coordinates, extract particle stack, change particle file format, filter particles by model score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this guide\n",
    "\n",
    "We assume that the user already has a file containing their labeled particle coordinates (data/EMPIAR-10025/rawdata/particles.txt in this case), that the expected number of particles per micrograph is 300, and that the particle radius is ~74 Å (14 pixels after downsampling 8x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "Downsample the micrographs by a factor of 8 (to ~5.28 Å/pix) and also scale the labeled coordinates to match.\n",
    "\n",
    "__A note on downsampling:__ For this demo dataset, we downsample 8x, but this may not always be the best downsampling amount for your data. We recommend downsampling your data enough that the diameter of your particle fits within the receptive field of the CNN architecture you are using. In this tutorial, we use the default architecture of resnet8, which has a receptive field size of 71 pixels. If you are using a different architecture, this number is slightly different. See [here](https://github.com/tbepler/topaz#model-architectures) for architecture details.\n",
    "\n",
    "As a rule of thumb, downsampling to about 4-8 Å per pixel works well, but this may need to be adjusted for very large or very small particles to fit the classifier as described above.\n",
    "\n",
    "__Pretrained models:__ When using the pretrained models, they assume that microcraphs are downsampled to the 4-8 Å per pixel range. Generally speaking, ~4 Å/pix is better for small to medium sized particles, but ~8 Å/pix is better for large particles (>= 300 Å particle diameter). Very large particles may require additional downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m this is main!\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "    \n",
    "import sys\n",
    "import importlib\n",
    "import topaz\n",
    "from topaz.main import main\n",
    "\n",
    "# importlib.reload(topaz.main)\n",
    "import pdb\n",
    "main?\n",
    "\n",
    "# replace /miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py - DONE\n",
    "# figure out softlinking so that my repo changes lead to changes in the miniconda package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 8\n"
     ]
    }
   ],
   "source": [
    "# confirm i have gpus accessible here\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs:\", num_gpus)\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\u001b[0m(53)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     51 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0;34m'''this is main!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "import glob\n",
    "input_files = glob.glob('/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc')\n",
    "input_files = ' '.join(input_files)\n",
    "\n",
    "# create command\n",
    "cmd = f'''preprocess -s 8 -o{input_files}'''\n",
    "# cmd = f'''preprocess -s 8 -d -1 -o /fastData/topaz_tutorial_data/EMPIAR-10025/processed/m2/ {input_files}'''\n",
    "\n",
    "#topaz.main.main(cmd)\n",
    "# import pdb;breakpoint()\n",
    "# main(cmd)\n",
    "# topaz.main.main(cmd)\n",
    "# pdb.run(f'''topaz.main.main(cmd)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# processed: 14sep05c_c_00003gr_00014sq_00004hl_00004es_c\n",
      "# processed: 14sep05c_c_00003gr_00014sq_00005hl_00003es_c\n",
      "# processed: 14sep05c_c_00003gr_00014sq_00007hl_00004es_c\n",
      "# processed: 14sep05c_c_00003gr_00014sq_00011hl_00003es_c\n",
      "# processed: 14sep05c_c_00003gr_00015sq_00015hl_00002es_c\n",
      "# processed: 14sep05c_c_00003gr_00018sq_00008hl_00003es_c\n",
      "# processed: 14sep05c_c_00003gr_00018sq_00010hl_00005es_c\n",
      "# processed: 14sep05c_c_00003gr_00020sq_00011hl_00002es_c\n",
      "# processed: 14sep05c_c_00003gr_00020sq_00011hl_00004es_c\n",
      "# processed: 14sep05c_c_00004gr_00031sq_00002hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00031sq_00005hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00031sq_00010hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00007hl_00003es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00010hl_00003es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00029hl_00005es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00031hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00033hl_00005es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00037hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00037hl_00003es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00040hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00040hl_00004es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00041hl_00005es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00004hl_00003es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00005hl_00002es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00006hl_00002es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00008hl_00003es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00008hl_00004es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00009hl_00002es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00009hl_00004es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00014hl_00004es_c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# source activate topaz\n",
    "\n",
    "# we'll store the processed data in data/EMPIAR-10025/processed\n",
    "# so we need to make these directories first\n",
    "# mkdir -p data/EMPIAR-10025/processed\n",
    "# mkdir -p data/EMPIAR-10025/processed/micrographs\n",
    "\n",
    "# to run the preprocess command, we pass the input micrographs as command line arguments\n",
    "# preprocess will write the processed images to the directory specified with the -o argument\n",
    "# -s sets the downsampling amount (in this case, we downsample by a factor of 8)\n",
    "# -d/--device X sets preprocess to use GPU with ID X\n",
    "# -t/--num-workers X sets preprocess to use X processes (preprocesses X micrographs in parallel), this and GPU device are mutually exclusive\n",
    "# -v gives verbose output to track progress\n",
    "topaz preprocess -v -s 8 -o /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc\n",
    "\n",
    "# this command takes the particle coordinates matched to the original micrographs\n",
    "# and scales them by 1/8 (-s is downscaling)\n",
    "# the -x option applies upscaling instead\n",
    "topaz convert -s 8 -o /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/particles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model training\n",
    "\n",
    "Given the preprocessed micrographs and particle coordinates, we train the particle detection model using positive-unlabeled learning. This requires us to specify the expected number of particles per micrograph, which we set to 400 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t7.90563e-03\t304500\t38516850\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.126490094594963\n",
      "# Estimated max precision given pi and p_observed: 0.0625\n",
      "# If your adjusted precision is greater than 1.0 (especially on a test split), you have likely set pi too high.\n",
      "# minibatch_size=256, epoch_size=1000, num_epochs=10\n",
      "# Loaded 60 training micrographs with ~1500 labeled particles\n",
      "# Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt', test_images=None, test_targets=None, format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-binomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=0.0002, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/model', output='saved_models/EMPIAR-10025/model_training.txt', test_batch_size=1, func=<function main at 0x7fdb750f1a80>)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# BASIC GE-BINOMIAL\n",
    "# source activate topaz\n",
    "\n",
    "# first, make sure we have the folders where we want to put the saved models\n",
    "# store the saved models in saved_models/EMPIAR-10025\n",
    "mkdir -p saved_models\n",
    "mkdir -p saved_models/EMPIAR-10025\n",
    "\n",
    "# Now, we train the model\n",
    "\n",
    "# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\n",
    "# and --num-workers=8 to speed up training\n",
    "\n",
    "# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\n",
    "# To use a different GPU, set -d X where X is the GPU ID\n",
    "\n",
    "# the models will be saved to the saved_models/EMPIAR-10025 directory\n",
    "topaz train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/model \\\n",
    "            -o saved_models/EMPIAR-10025/model_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t7.90563e-03\t304500\t38516850\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.126490094594963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt', test_images=None, test_targets=None, format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-multinomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=0.0002, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/model_multinomial', output='saved_models/EMPIAR-10025/model_multinomial_training.txt', test_batch_size=1, func=<function main at 0x7faadb7f5a80>)\n",
      "PI DEVICE:cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/bin/topaz\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('topaz-em==0.3.7', 'console_scripts', 'topaz')())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\", line 152, in main\n",
      "    args.func(args)\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/commands/train.py\", line 137, in main\n",
      "    classifier = train_model(classifier, args.train_images, args.train_targets, args.test_images, args.test_targets, \n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/training.py\", line 629, in train_model\n",
      "    trainer, criteria, split = make_training_step_method(classifier, num_positive_regions,\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/training.py\", line 402, in make_training_step_method\n",
      "    trainer = methods.GE_multinomial(classifier, optim, criteria, pi, l2=l2, slack=slack, autoencoder=autoencoder)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/methods.py\", line 357, in __init__\n",
      "    print('Sigma 2:' + Sigma_2.get_device())\n",
      "                       ^^^^^^^\n",
      "NameError: name 'Sigma_2' is not defined. Did you mean: 'self.Sigma_2'?\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"# TEST GE-MULTINOMIAL\\n# source activate topaz\\n\\n# first, make sure we have the folders where we want to put the saved models\\n# store the saved models in saved_models/EMPIAR-10025\\n# mkdir -p saved_models\\n# mkdir -p saved_models/EMPIAR-10025\\n\\n# Now, we train the model\\n\\n# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\\n# and --num-workers=8 to speed up training\\n\\n# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\\n# To use a different GPU, set -d X where X is the GPU ID\\n\\n# the models will be saved to the saved_models/EMPIAR-10025 directory\\ntopaz train -n 400 \\\\\\n            --num-workers=8 \\\\\\n            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\\\\n            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\\\\n            --method='GE-multinomial' \\\\\\n            --save-prefix=saved_models/EMPIAR-10025/model_multinomial \\\\\\n            -o saved_models/EMPIAR-10025/model_multinomial_training.txt\\n\"' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbash\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# TEST GE-MULTINOMIAL\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# source activate topaz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# first, make sure we have the folders where we want to put the saved models\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# store the saved models in saved_models/EMPIAR-10025\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# mkdir -p saved_models\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# mkdir -p saved_models/EMPIAR-10025\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# Now, we train the model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# and --num-workers=8 to speed up training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# To use a different GPU, set -d X where X is the GPU ID\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m# the models will be saved to the saved_models/EMPIAR-10025 directory\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtopaz train -n 400 \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            --num-workers=8 \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            --method=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGE-multinomial\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            --save-prefix=saved_models/EMPIAR-10025/model_multinomial \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m            -o saved_models/EMPIAR-10025/model_multinomial_training.txt\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshebang(line, cell)\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"# TEST GE-MULTINOMIAL\\n# source activate topaz\\n\\n# first, make sure we have the folders where we want to put the saved models\\n# store the saved models in saved_models/EMPIAR-10025\\n# mkdir -p saved_models\\n# mkdir -p saved_models/EMPIAR-10025\\n\\n# Now, we train the model\\n\\n# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\\n# and --num-workers=8 to speed up training\\n\\n# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\\n# To use a different GPU, set -d X where X is the GPU ID\\n\\n# the models will be saved to the saved_models/EMPIAR-10025 directory\\ntopaz train -n 400 \\\\\\n            --num-workers=8 \\\\\\n            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\\\\n            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\\\\n            --method='GE-multinomial' \\\\\\n            --save-prefix=saved_models/EMPIAR-10025/model_multinomial \\\\\\n            -o saved_models/EMPIAR-10025/model_multinomial_training.txt\\n\"' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# TEST GE-MULTINOMIAL\n",
    "# source activate topaz\n",
    "\n",
    "# first, make sure we have the folders where we want to put the saved models\n",
    "# store the saved models in saved_models/EMPIAR-10025\n",
    "# mkdir -p saved_models\n",
    "# mkdir -p saved_models/EMPIAR-10025\n",
    "\n",
    "# Now, we train the model\n",
    "\n",
    "# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\n",
    "# and --num-workers=8 to speed up training\n",
    "\n",
    "# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\n",
    "# To use a different GPU, set -d X where X is the GPU ID\n",
    "\n",
    "# the models will be saved to the saved_models/EMPIAR-10025 directory\n",
    "topaz train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\n",
    "            --method='GE-multinomial' \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/model_multinomial \\\n",
    "            -o saved_models/EMPIAR-10025/model_multinomial_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m this is main!\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "    \n",
    "import sys\n",
    "import importlib\n",
    "import topaz\n",
    "from topaz.main import main\n",
    "\n",
    "# importlib.reload(topaz.main)\n",
    "import pdb\n",
    "main?\n",
    "\n",
    "# replace /miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py - DONE\n",
    "# figure out softlinking so that my repo changes lead to changes in the miniconda package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\u001b[0m(53)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     51 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0;34m'''this is main!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt', test_images=None, test_targets=None, format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-multinomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=0.0002, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/model_multinomial', output='saved_models/EMPIAR-10025/model_multinomial_training.txt', test_batch_size=1, func=<function main at 0x7f6929d20b80>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t7.90563e-03\t304500\t38516850\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.126490094594963\n",
      "# Estimated max precision given pi and p_observed: 0.0625\n",
      "# If your adjusted precision is greater than 1.0 (especially on a test split), you have likely set pi too high.\n",
      "# minibatch_size=256, epoch_size=1000, num_epochs=10\n",
      "# Loaded 60 training micrographs with ~1500 labeled particles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6012, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2169, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1415, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0707, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1690, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1043, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2258, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1578, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1942, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2710, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4354, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2134, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1248, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1838, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1298, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1320, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1432, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1789, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0997, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2049, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1214, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1431, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1235, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0534, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2203, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1046, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2156, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0702, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1296, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1790, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2697, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1141, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1806, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1389, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0940, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1696, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1564, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2803, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0675, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1326, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0525, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1057, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1378, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0585, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1006, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1683, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1855, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0278, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "## test GE_multinomial on pdb\n",
    "import glob\n",
    "input_files = glob.glob('/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc')\n",
    "input_files = ' '.join(input_files)\n",
    "\n",
    "# create command\n",
    "cmd = f'''train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\n",
    "            --method='GE-multinomial' \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/model_multinomial \\\n",
    "            -o saved_models/EMPIAR-10025/model_multinomial_training.txt'''\n",
    "## AFTER --method can add             # --learning-rate=0.00002 \\\n",
    "#topaz.main.main(cmd)\n",
    "# import pdb;breakpoint()\n",
    "# main(cmd)\n",
    "# topaz.main.main(cmd)\n",
    "pdb.run(f'''topaz.main.main(cmd)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract particle coordinates\n",
    "\n",
    "Now that we have a trained model, we use it to extract predicted particle coordinates using a particle radius of 14 pixels.\n",
    "\n",
    "Extract can be run using a particle picking model trained above by passing the model path as an argument. It can also use the pretrained models by specifying on of:\n",
    "\n",
    "- resnet8_u32\n",
    "- resnet8_u64\n",
    "- resnet16_u32\n",
    "- resnet16_u64 (the default)\n",
    "\n",
    "as the model argument. If no argument is passed, extract uses the resnet16_u64 pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# source activate topaz\n",
    "\n",
    "## make a directory to write the topaz particles to\n",
    "mkdir -p data/EMPIAR-10025/topaz\n",
    "\n",
    "## extract particle coordinates using the  trained model\n",
    "## we set the radius parameter to 14 (-r 14)\n",
    "## to prevent extracting particle coordinates closer than the radius of the particle\n",
    "## i.e. we don't want multiple predictions for a single particle\n",
    "## we also set -x 8 in order to scale the coordinates back to the original micrograph size\n",
    "\n",
    "topaz extract -r 14 -x 8 -m saved_models/EMPIAR-10025/model_epoch10.sav \\\n",
    "              -o data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt \\\n",
    "              data/EMPIAR-10025/processed/micrographs/*.mrc\n",
    "              \n",
    "              \n",
    "## To use the pretrained particle picking model instead, we can omit the model argument.\n",
    "\n",
    "#topaz extract -r 14 -x 8 \\\n",
    "#              -o data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt \\\n",
    "#              data/EMPIAR-10025/processed/micrographs/*.mrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) change format of particle coordinates file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate topaz\n",
    "\n",
    "# we can convert the particles file to .star format (and others) by changing the file extension\n",
    "# of the output file (data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt)\n",
    "# to .star (data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.star)\n",
    "# the convert command can also filter the particle table by model score using the -t argument\n",
    "# e.g. -t 0 would only keep particles with scores >= 0\n",
    "\n",
    "topaz convert -o data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.star \\\n",
    "              data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "We now have a table containing particle coordinates for each micrograph with their corresponding model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- To better understand the outputs of the individual steps and to visualize intermediate results, take a look at the detailed walkthrough [here](https://github.com/tbepler/topaz/blob/master/tutorial/02_walkthrough.ipynb)\n",
    "\n",
    "- To jump straight to understanding model selection and evaluation criteria, take a look at the cross validation tutorial [here](https://github.com/tbepler/topaz/blob/master/tutorial/03_cross_validation.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

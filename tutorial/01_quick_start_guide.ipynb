{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Topaz pipeline\n",
    "\n",
    "The Topaz particle picking pipeline proceeds as follows:\n",
    "\n",
    "(Before running Topaz, optional): label a small (100-1000, more is likely to give better results) number of particles on your micrographs using the software of your choice. This can be skipped when using the pretrained models bundled with Topaz. \n",
    "\n",
    "1. (Preprocessing) Micrographs are downsampled and normalized, any labeled particle coordinates also need to be scaled appropriately\n",
    "2. (Training, optional) The particle detection model is trained on the preprocessed micrographs using the labeled particle coordinates. This requires setting the expected number of particles per micrograph. This can be skipped when using the pretrained models bundled with Topaz.\n",
    "3. (Extraction) Using a trained model, particle coordinates and their associated scores are extracted from the micrographs. This requires knowing the particle radius in pixels on the downsampled micrographs.\n",
    "\n",
    "(Optional postprocessing): examine classifier performance, rescale particle coordinates, extract particle stack, change particle file format, filter particles by model score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this guide\n",
    "\n",
    "We assume that the user already has a file containing their labeled particle coordinates (data/EMPIAR-10025/rawdata/particles.txt in this case), that the expected number of particles per micrograph is 300, and that the particle radius is ~74 Å (14 pixels after downsampling 8x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "Downsample the micrographs by a factor of 8 (to ~5.28 Å/pix) and also scale the labeled coordinates to match.\n",
    "\n",
    "__A note on downsampling:__ For this demo dataset, we downsample 8x, but this may not always be the best downsampling amount for your data. We recommend downsampling your data enough that the diameter of your particle fits within the receptive field of the CNN architecture you are using. In this tutorial, we use the default architecture of resnet8, which has a receptive field size of 71 pixels. If you are using a different architecture, this number is slightly different. See [here](https://github.com/tbepler/topaz#model-architectures) for architecture details.\n",
    "\n",
    "As a rule of thumb, downsampling to about 4-8 Å per pixel works well, but this may need to be adjusted for very large or very small particles to fit the classifier as described above.\n",
    "\n",
    "__Pretrained models:__ When using the pretrained models, they assume that microcraphs are downsampled to the 4-8 Å per pixel range. Generally speaking, ~4 Å/pix is better for small to medium sized particles, but ~8 Å/pix is better for large particles (>= 300 Å particle diameter). Very large particles may require additional downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m this is main!\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "    \n",
    "import sys\n",
    "import importlib\n",
    "import topaz\n",
    "from topaz.main import main\n",
    "\n",
    "# importlib.reload(topaz.main)\n",
    "import pdb\n",
    "main?\n",
    "\n",
    "# replace /miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py - DONE\n",
    "# figure out softlinking so that my repo changes lead to changes in the miniconda package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 8\n"
     ]
    }
   ],
   "source": [
    "# confirm i have gpus accessible here\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs:\", num_gpus)\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\u001b[0m(53)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     51 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0;34m'''this is main!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "import glob\n",
    "input_files = glob.glob('/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc')\n",
    "input_files = ' '.join(input_files)\n",
    "\n",
    "# create command\n",
    "cmd = f'''preprocess -s 8 -o{input_files}'''\n",
    "# cmd = f'''preprocess -s 8 -d -1 -o /fastData/topaz_tutorial_data/EMPIAR-10025/processed/m2/ {input_files}'''\n",
    "\n",
    "#topaz.main.main(cmd)\n",
    "# import pdb;breakpoint()\n",
    "# main(cmd)\n",
    "# topaz.main.main(cmd)\n",
    "# pdb.run(f'''topaz.main.main(cmd)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# processed: 14sep05c_c_00003gr_00014sq_00004hl_00004es_c\n",
      "# processed: 14sep05c_c_00003gr_00014sq_00005hl_00003es_c\n",
      "# processed: 14sep05c_c_00003gr_00014sq_00007hl_00004es_c\n",
      "# processed: 14sep05c_c_00003gr_00014sq_00011hl_00003es_c\n",
      "# processed: 14sep05c_c_00003gr_00015sq_00015hl_00002es_c\n",
      "# processed: 14sep05c_c_00003gr_00018sq_00008hl_00003es_c\n",
      "# processed: 14sep05c_c_00003gr_00018sq_00010hl_00005es_c\n",
      "# processed: 14sep05c_c_00003gr_00020sq_00011hl_00002es_c\n",
      "# processed: 14sep05c_c_00003gr_00020sq_00011hl_00004es_c\n",
      "# processed: 14sep05c_c_00004gr_00031sq_00002hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00031sq_00005hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00031sq_00010hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00007hl_00003es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00010hl_00003es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00029hl_00005es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00031hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00033hl_00005es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00037hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00037hl_00003es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00040hl_00002es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00040hl_00004es_c\n",
      "# processed: 14sep05c_c_00004gr_00032sq_00041hl_00005es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00004hl_00003es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00005hl_00002es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00006hl_00002es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00008hl_00003es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00008hl_00004es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00009hl_00002es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00009hl_00004es_c\n",
      "# processed: 14sep05c_c_00007gr_00013sq_00014hl_00004es_c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# source activate topaz\n",
    "\n",
    "# we'll store the processed data in data/EMPIAR-10025/processed\n",
    "# so we need to make these directories first\n",
    "# mkdir -p data/EMPIAR-10025/processed\n",
    "# mkdir -p data/EMPIAR-10025/processed/micrographs\n",
    "\n",
    "# to run the preprocess command, we pass the input micrographs as command line arguments\n",
    "# preprocess will write the processed images to the directory specified with the -o argument\n",
    "# -s sets the downsampling amount (in this case, we downsample by a factor of 8)\n",
    "# -d/--device X sets preprocess to use GPU with ID X\n",
    "# -t/--num-workers X sets preprocess to use X processes (preprocesses X micrographs in parallel), this and GPU device are mutually exclusive\n",
    "# -v gives verbose output to track progress\n",
    "topaz preprocess -v -s 8 -o /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc\n",
    "\n",
    "# this command takes the particle coordinates matched to the original micrographs\n",
    "# and scales them by 1/8 (-s is downscaling)\n",
    "# the -x option applies upscaling instead\n",
    "topaz convert -s 8 -o /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/particles.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model training\n",
    "\n",
    "Given the preprocessed micrographs and particle coordinates, we train the particle detection model using positive-unlabeled learning. This requires us to specify the expected number of particles per micrograph, which we set to 400 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t7.90563e-03\t304500\t38516850\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.126490094594963\n",
      "# Estimated max precision given pi and p_observed: 0.0625\n",
      "# If your adjusted precision is greater than 1.0 (especially on a test split), you have likely set pi too high.\n",
      "# minibatch_size=256, epoch_size=1000, num_epochs=10\n",
      "# Loaded 60 training micrographs with ~1500 labeled particles\n",
      "# Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt', test_images=None, test_targets=None, format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-binomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=0.0002, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/model', output='saved_models/EMPIAR-10025/model_training.txt', test_batch_size=1, func=<function main at 0x7fdb750f1a80>)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# BASIC GE-BINOMIAL\n",
    "# source activate topaz\n",
    "\n",
    "# first, make sure we have the folders where we want to put the saved models\n",
    "# store the saved models in saved_models/EMPIAR-10025\n",
    "mkdir -p saved_models\n",
    "mkdir -p saved_models/EMPIAR-10025\n",
    "\n",
    "# Now, we train the model\n",
    "\n",
    "# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\n",
    "# and --num-workers=8 to speed up training\n",
    "\n",
    "# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\n",
    "# To use a different GPU, set -d X where X is the GPU ID\n",
    "\n",
    "# the models will be saved to the saved_models/EMPIAR-10025 directory\n",
    "topaz train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/model \\\n",
    "            -o saved_models/EMPIAR-10025/model_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t1.14174e-02\t203000\t17779860\n",
      "# 0\ttest\t1.14174e-02\t101500\t8889930\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.09133930188426681\n",
      "# Estimated max precision given pi and p_observed: 0.125\n",
      "# If your adjusted precision is greater than 1.0 (especially on a test split), you have likely set pi too high.\n",
      "# minibatch_size=256, epoch_size=1000, num_epochs=10\n",
      "# Loaded 20 training micrographs with ~1000 labeled particles\n",
      "# Loaded 10 testing micrographs with 500 labeled particles\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "RuntimeWarning: overflow encountered in exp\n",
      "# Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/image_list_train.txt', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/14sep05c_c_00007gr_00013sq_00004hl_00003es_c_train.txt', test_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/image_list_test.txt', test_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/14sep05c_c_00007gr_00013sq_00004hl_00003es_c_test.txt', format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-binomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=0.0002, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/binomial_KLonly/model', output='saved_models/EMPIAR-10025/binomial_KLonly/model_training.txt', test_batch_size=1, func=<function main at 0x7f4849475b20>)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# from 02_walkthrough! vanilla GE-binomial model - KL div ONLY\n",
    "topaz train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/image_list_train.txt \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/14sep05c_c_00007gr_00013sq_00004hl_00003es_c_train.txt \\\n",
    "            --test-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/image_list_test.txt \\\n",
    "            --test-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/14sep05c_c_00007gr_00013sq_00004hl_00003es_c_test.txt \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/binomial_KLonly/model \\\n",
    "            -o saved_models/EMPIAR-10025/binomial_KLonly/model_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t7.90563e-03\t304500\t38516850\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.126490094594963\n",
      "# Estimated max precision given pi and p_observed: 0.0625\n",
      "# If your adjusted precision is greater than 1.0 (especially on a test split), you have likely set pi too high.\n",
      "# minibatch_size=256, epoch_size=1000, num_epochs=10\n",
      "# Loaded 60 training micrographs with ~1500 labeled particles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt', test_images=None, test_targets=None, format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-multinomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=5e-05, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/multinomial_xentropy/model_multinomial', output='saved_models/EMPIAR-10025/multinomial_xentropy/model_multinomial_training.txt', test_batch_size=1, func=<function main at 0x7f23d68e9a80>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/bin/topaz\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('topaz-em==0.3.7', 'console_scripts', 'topaz')())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\", line 152, in main\n",
      "    args.func(args)\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/commands/train.py\", line 137, in main\n",
      "    classifier = train_model(classifier, args.train_images, args.train_targets, args.test_images, args.test_targets, \n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/training.py\", line 651, in train_model\n",
      "    fit_epochs(classifier, criteria, trainer, train_iterator, test_iterator, args.num_epochs, est_max_prec,\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/training.py\", line 590, in fit_epochs\n",
      "    it = fit_epoch(step_method, train_iterator, est_max_prec=est_max_prec, epoch=epoch, it=it, use_cuda=use_cuda, output=output)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/training.py\", line 560, in fit_epoch\n",
      "    for X,Y in data_iterator:\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/utils/data/memory_mapped_data.py\", line 214, in __getitem__\n",
      "    crop,label = img.get_UN_crop(), 0.\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/utils/data/memory_mapped_data.py\", line 100, in get_UN_crop\n",
      "    return self.get_crop((z, y, x))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/utils/data/memory_mapped_data.py\", line 56, in get_crop\n",
      "    with open(self.image_path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# TEST GE-MULTINOMIAL\n",
    "# source activate topaz\n",
    "\n",
    "# first, make sure we have the folders where we want to put the saved models\n",
    "# store the saved models in saved_models/EMPIAR-10025\n",
    "# mkdir -p saved_models\n",
    "# mkdir -p saved_models/EMPIAR-10025\n",
    "\n",
    "# Now, we train the model\n",
    "\n",
    "# We set -n 400 to tell Topaz that we expect there to be on average 400 particles per micrograph\n",
    "# and --num-workers=8 to speed up training\n",
    "\n",
    "# By default, topaz train will use your first GPU if available, to force topaz train to use the CPU, set: -d -1\n",
    "# To use a different GPU, set -d X where X is the GPU ID\n",
    "\n",
    "# the models will be saved to the saved_models/EMPIAR-10025 directory\n",
    "topaz train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\n",
    "            --method='GE-multinomial' \\\n",
    "            --learning-rate=0.00005 \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/multinomial_xentropy/model_multinomial \\\n",
    "            -o saved_models/EMPIAR-10025/multinomial_xentropy/model_multinomial_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5)\n",
    "y = torch.tensor([0,1,0,1,0],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7926)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(probs)\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss(log_probs,y))\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1186\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1187\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/topaz/lib/python3.12/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "# GE-binomial loss\n",
    "loss2 = torch.nn.BCEWithLogitsLoss()\n",
    "print(loss2(x,y))\n",
    "\n",
    "# Replicate with CEL\n",
    "probs = torch.sigmoid(x)\n",
    "probs = torch.column_stack((1-probs,probs))\n",
    "log_probs = torch.log(probs)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "print(loss(log_probs,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m this is main!\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "    \n",
    "import sys\n",
    "import importlib\n",
    "import topaz\n",
    "from topaz.main import main\n",
    "\n",
    "# importlib.reload(topaz.main)\n",
    "import pdb\n",
    "main?\n",
    "\n",
    "# replace /miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py - DONE\n",
    "# figure out softlinking so that my repo changes lead to changes in the miniconda package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\u001b[0m(53)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     51 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0;34m'''this is main!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(command='train', describe=False, device=0, num_workers=8, num_threads=0, train_images='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/', train_targets='/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt', test_images=None, test_targets=None, format_='auto', image_ext='', k_fold=0, fold=0, cross_validation_seed=42, num_particles=400.0, pi=None, radius=3, method='GE-multinomial', slack=-1, autoencoder=0, l2=0.0, learning_rate=0.0002, natural=False, minibatch_size=256, minibatch_balance=0.0625, epoch_size=1000, num_epochs=10, pretrained=True, model='resnet8', units=32, dropout=0.0, bn='on', pooling=None, unit_scaling=2, ngf=32, patch_size=96, patch_padding=48, save_prefix='saved_models/EMPIAR-10025/model_multinomial', output='saved_models/EMPIAR-10025/model_multinomial_training.txt', test_batch_size=1, func=<function main at 0x7efe2c808860>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading model: resnet8\n",
      "# Model parameters: units=32, dropout=0.0, bn=on\n",
      "# Loading pretrained model: resnet8_u32\n",
      "# Receptive field: 71\n",
      "# Using device=0 with cuda=True\n",
      "# When using GPU to load data, we only load in this process. Setting num_workers = 0.\n",
      "# Training...\n",
      "# source\tsplit\tp_observed\tnum_positive_regions\ttotal_regions\n",
      "# 0\ttrain\t7.90563e-03\t304500\t38516850\n",
      "# Specified expected number of particle per micrograph = 400.0\n",
      "# With radius = 3\n",
      "# Setting pi = 0.126490094594963\n",
      "# Estimated max precision given pi and p_observed: 0.0625\n",
      "# If your adjusted precision is greater than 1.0 (especially on a test split), you have likely set pi too high.\n",
      "# minibatch_size=256, epoch_size=1000, num_epochs=10\n",
      "# Loaded 60 training micrographs with ~1500 labeled particles\n",
      "# Done!\n"
     ]
    }
   ],
   "source": [
    "# %debug\n",
    "## test GE_multinomial on pdb\n",
    "import glob\n",
    "input_files = glob.glob('/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc')\n",
    "input_files = ' '.join(input_files)\n",
    "\n",
    "# create command\n",
    "cmd = f'''train -n 400 \\\n",
    "            --num-workers=8 \\\n",
    "            --train-images /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ \\\n",
    "            --train-targets /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt \\\n",
    "            --method='GE-multinomial' \\\n",
    "            --save-prefix=saved_models/EMPIAR-10025/model_multinomial \\\n",
    "            -o saved_models/EMPIAR-10025/model_multinomial_training.txt'''\n",
    "## AFTER --method can add             # --learning-rate=0.00002 \\\n",
    "#topaz.main.main(cmd)\n",
    "# import pdb;breakpoint()\n",
    "# main(cmd)\n",
    "# topaz.main.main(cmd)\n",
    "pdb.run(f'''topaz.main.main(cmd)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract particle coordinates\n",
    "\n",
    "Now that we have a trained model, we use it to extract predicted particle coordinates using a particle radius of 14 pixels.\n",
    "\n",
    "Extract can be run using a particle picking model trained above by passing the model path as an argument. It can also use the pretrained models by specifying on of:\n",
    "\n",
    "- resnet8_u32\n",
    "- resnet8_u64\n",
    "- resnet16_u32\n",
    "- resnet16_u64 (the default)\n",
    "\n",
    "as the model argument. If no argument is passed, extract uses the resnet16_u64 pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# source activate topaz\n",
    "\n",
    "## make a directory to write the topaz particles to\n",
    "mkdir -p data/EMPIAR-10025/topaz\n",
    "\n",
    "## extract particle coordinates using the  trained model\n",
    "## we set the radius parameter to 14 (-r 14)\n",
    "## to prevent extracting particle coordinates closer than the radius of the particle\n",
    "## i.e. we don't want multiple predictions for a single particle\n",
    "## we also set -x 8 in order to scale the coordinates back to the original micrograph size\n",
    "\n",
    "topaz extract -r 14 -x 8 -m saved_models/EMPIAR-10025/model_epoch10.sav \\\n",
    "              -o data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt \\\n",
    "              data/EMPIAR-10025/processed/micrographs/*.mrc\n",
    "              \n",
    "              \n",
    "## To use the pretrained particle picking model instead, we can omit the model argument.\n",
    "\n",
    "#topaz extract -r 14 -x 8 \\\n",
    "#              -o data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt \\\n",
    "#              data/EMPIAR-10025/processed/micrographs/*.mrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) change format of particle coordinates file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate topaz\n",
    "\n",
    "# we can convert the particles file to .star format (and others) by changing the file extension\n",
    "# of the output file (data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt)\n",
    "# to .star (data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.star)\n",
    "# the convert command can also filter the particle table by model score using the -t argument\n",
    "# e.g. -t 0 would only keep particles with scores >= 0\n",
    "\n",
    "topaz convert -o data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.star \\\n",
    "              data/EMPIAR-10025/topaz/predicted_particles_all_upsampled.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it!\n",
    "\n",
    "We now have a table containing particle coordinates for each micrograph with their corresponding model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- To better understand the outputs of the individual steps and to visualize intermediate results, take a look at the detailed walkthrough [here](https://github.com/tbepler/topaz/blob/master/tutorial/02_walkthrough.ipynb)\n",
    "\n",
    "- To jump straight to understanding model selection and evaluation criteria, take a look at the cross validation tutorial [here](https://github.com/tbepler/topaz/blob/master/tutorial/03_cross_validation.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

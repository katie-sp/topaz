diff --git a/topaz/commands/train.py b/topaz/commands/train.py
index 66b69ed..acf777a 100644
--- a/topaz/commands/train.py
+++ b/topaz/commands/train.py
@@ -61,7 +61,7 @@ def add_arguments(parser=None):
     # training parameters
     training.add_argument('-r', '--radius', default=3, type=int, help='pixel radius around particle centers to consider positive (default: 3)')
 
-    methods = ['PN', 'GE-KL', 'GE-binomial', 'PU']
+    methods = ['PN', 'GE-KL', 'GE-binomial', 'PU', 'GE-multinomial']
     training.add_argument('--method', choices=methods, default='GE-binomial', help='objective function to use for learning the region classifier (default: GE-binomial)')
     training.add_argument('--slack', default=-1, type=float, help='weight on GE penalty (default: 10 for GE-KL, 1 for GE-binomial)')
 
diff --git a/topaz/main.py b/topaz/main.py
index 4ceb585..72fcd86 100644
--- a/topaz/main.py
+++ b/topaz/main.py
@@ -50,7 +50,8 @@ def generate_description(module_groups, linewidth=78, indent='  ', delim='  '):
     return '\n'.join(description)
 
 
-def main():
+def main(cmd=None):
+    '''this is main!'''
     import argparse
     parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, fromfile_prefix_chars='@')
 
@@ -124,7 +125,6 @@ def main():
                       ]
                      ),
                     ]
-
     description = generate_description(module_groups)
     
     subparsers = parser.add_subparsers(title='commands', metavar='<command>'
@@ -142,9 +142,13 @@ def main():
     #subparsers.required = 'True'
     #subparsers.dest = 'command'
 
-
-    args = parser.parse_args()
-
+    if cmd is None:
+        args = parser.parse_args()
+    else:
+        import shlex
+        cmd_args = shlex.split(cmd)
+        args = parser.parse_args(cmd_args)
+    print(args)
     args.func(args)
 
 
diff --git a/topaz/main_ORIGINAL.py b/topaz/main_ORIGINAL.py
new file mode 100644
index 0000000..4ceb585
--- /dev/null
+++ b/topaz/main_ORIGINAL.py
@@ -0,0 +1,154 @@
+from __future__ import absolute_import, print_function, division
+
+def generate_description(module_groups, linewidth=78, indent='  ', delim='  '):
+
+    """
+    description = []
+    for group,module_list in module_groups:
+        description.append(group + ':')
+        for module in module_list:
+            description.append('  ' + module.name + '\t' + module.help)
+        description.append('')
+    description = '\n'.join(description)
+    """
+
+    description = []
+
+    names = []
+    for group,module_list in module_groups:
+        for module in module_list:
+            names.append(module.name)
+
+    ## name column width
+    name_width = max(len(name) for name in names)
+    desc_width = linewidth - len(indent) - name_width - len(delim)
+    
+    for group,module_list in module_groups:
+        description.append(group + ':')
+        for module in module_list:
+            name = module.name
+            descriptor = module.help
+            ## first line includes name, pad to name_width
+            name = name + ' '*(name_width-len(name))
+            ## take tokens from descriptor up to desc_width to generate lines
+            width = 0
+            line_tokens = []
+            for token in descriptor.split():
+                if width + len(token) > desc_width:
+                    if width > 0: # push current tokens to line
+                        line = indent + name + delim + ' '.join(line_tokens)
+                        description.append(line)
+                        name = ' '*name_width
+                    width = 0
+                    line_tokens = []
+                line_tokens.append(token)
+                width += len(token)
+            if width > 0:
+                line = indent + name + delim + ' '.join(line_tokens)
+                description.append(line)
+        description.append('')
+    return '\n'.join(description)
+
+
+def main():
+    import argparse
+    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, fromfile_prefix_chars='@')
+
+    import topaz
+    parser.add_argument('--version', action='version', version=topaz.__version__)
+
+    import topaz.commands.train
+    import topaz.commands.segment
+    import topaz.commands.extract
+    import topaz.commands.precision_recall_curve
+
+    import topaz.commands.downsample
+    import topaz.commands.normalize
+    import topaz.commands.preprocess
+    import topaz.commands.denoise
+    import topaz.commands.denoise3d
+
+    import topaz.commands.gui
+
+    import topaz.commands.convert
+    import topaz.commands.split
+    import topaz.commands.particle_stack
+    import topaz.commands.train_test_split
+
+    # deprecated
+    import topaz.commands.scale_coordinates
+    import topaz.commands.boxes_to_coordinates 
+    import topaz.commands.star_to_coordinates 
+    import topaz.commands.coordinates_to_star
+    import topaz.commands.coordinates_to_boxes
+    import topaz.commands.coordinates_to_eman2_json
+    import topaz.commands.star_particles_threshold
+
+    module_groups = [('Particle picking',
+                      [topaz.commands.train,
+                       topaz.commands.segment,
+                       topaz.commands.extract,
+                       topaz.commands.precision_recall_curve,
+                      ]
+                     ),
+                     ('Image processing',
+                      [topaz.commands.downsample,
+                       topaz.commands.normalize,
+                       topaz.commands.preprocess,
+                       topaz.commands.denoise,
+                       topaz.commands.denoise3d,
+                      ]
+                     ),
+                     ('File utilities',
+                      [
+                       topaz.commands.convert,
+                       topaz.commands.split,
+                       topaz.commands.particle_stack,
+                       topaz.commands.train_test_split,
+                      ]
+                     ),
+                     ('GUI',
+                      [
+                       topaz.commands.gui,
+                      ]
+                     ),
+                     ('[Deprecated]',
+                      [
+                       topaz.commands.scale_coordinates,
+                       topaz.commands.boxes_to_coordinates,
+                       topaz.commands.star_to_coordinates,
+                       topaz.commands.coordinates_to_star,
+                       topaz.commands.coordinates_to_boxes,
+                       topaz.commands.coordinates_to_eman2_json,
+                       topaz.commands.star_particles_threshold,
+                      ]
+                     ),
+                    ]
+
+    description = generate_description(module_groups)
+    
+    subparsers = parser.add_subparsers(title='commands', metavar='<command>'
+                                      , description=description)
+    subparsers.required = 'True'
+    subparsers.dest = 'command'
+    for group,module_list in module_groups:
+        for module in module_list:
+            this_parser = subparsers.add_parser(module.name) #, help=module.help)
+            module.add_arguments(this_parser)
+            this_parser.set_defaults(func=module.main)
+
+
+    #subparsers = parser.add_subparsers(title='Conversion utilities', metavar='<command>')
+    #subparsers.required = 'True'
+    #subparsers.dest = 'command'
+
+
+    args = parser.parse_args()
+
+    args.func(args)
+
+
+if __name__ == '__main__':
+    main()
+
+
diff --git a/topaz/methods.py b/topaz/methods.py
index 3c43b3e..f829d24 100644
--- a/topaz/methods.py
+++ b/topaz/methods.py
@@ -321,3 +321,99 @@ class PU:
 
         return (loss.item(),precision,tpr,fpr)
 
+class GE_multinomial:
+    ''' implemented by Katie, Jan 2025 '''
+    def __init__(self, model, optim, criteria, pi, l2=0
+                , slack=1.0 #, labeled_fraction=0
+                , entropy_penalty=0
+                , autoencoder=0
+                , posterior_L1=0):
+        self.model = model
+        self.optim = optim
+        self.criteria = criteria  # should be nn.NLLLoss (negative log likelihood, apt for multiclass)
+        self.slack = slack
+        pi = torch.tensor((pi,), type=torch.float32) if np.isscalar(pi) else pi
+        self.pi = pi # expected frequencies of each particle type in the unlabeled region; these should not include particles from the labeled region
+        self.entropy_penalty = entropy_penalty
+        #self.labeled_fraction = labeled_fraction
+        self.l2 = l2
+        self.autoencoder = autoencoder
+        self.posterior_L1 = posterior_L1
+
+        self.header = ['loss', 'ge_penalty', 'precision', 'adjusted_precision', 'tpr', 'fpr']
+        if self.autoencoder > 0:
+            self.header = ['loss', 'ge_penalty', 'recon_error', 'precision', 'adjusted_precision', 'tpr', 'fpr']
+
+        # Katie: compute Sigma_2, the covariance matrix of class frequencies, as well as its inverse and determinant
+        self.Sigma_2 = -torch.outer(pi, pi)
+        self.Sigma_2[range(len(pi)), range(len(pi))] = pi*(1-pi)  # Binomial variance down diagonal
+        prob_neg = 1 - pi.sum()
+        self.Sigma_2_det = (torch.prod(pi)*prob_neg).item()  # torch.det(self.Sigma_2).item()
+        self.Sigma_2_inverse = 1/prob_neg * torch.ones(*self.Sigma_2.shape)  # torch.linalg.inv(self.Sigma_2)
+        self.Sigma_2_inverse[range(len(self.Sigma_2)), range(len(self.Sigma_2))] = 1/pi + 1/prob_neg
+
+    def step(self, X, Y):
+        ''' 
+        X is the batch with features
+        Y is the class indices of each particle type; the first index (0) corresponds to the unlabeled regions), similar to GE-binomial
+        '''
+        if self.autoencoder > 0:
+            recon_error, score = autoencoder_loss(self.model, X)
+        else:
+            score = self.model(X).squeeze()
+
+        ## select data in the labeled region and calculate classifier loss for predicting particle types
+        ## score is a 2darray with each row representing a vector of scores for each particle type
+
+        ## first, convert scores to probabilities
+        if score.ndim == 1:  ## special case: binomial
+            probs = torch.sigmoid(score)
+            probs = torch.column_stack((1-probs, probs))  # prepend probabilty of not getting a particle
+        else:
+            probs = F.softmax(score, -1)
+        log_probs = torch.log(probs)
+        Y = Y.to(torch.long) # make sure Y is integer index
+        select = (Y.data > 0)
+        classifier_loss = self.criteria(log_probs[select], Y[select])
+
+        ## calculate GE penalty as divergence of empirical distribution and multinomial distribution/prior
+        p_hat = probs[~select, 1:]  # discard junk column
+        Sigma = -torch.einsum('pi,pj->ij',p_hat,p_hat)/p_hat.shape[0]
+        Sigma_1[range(Sigma_1.shape[0]),range(Sigma_1.shape[1])] = (p_hat*(1-p_hat)).mean(0)
+        mu_diff = p_hat.mean(0) - self.pi
+
+        ## GE penalty is the KL divergence of two multivariate normals
+        ge_penalty = 0.5*((self.Sigma_2_inverse @ Sigma).trace() + \
+                mu_diff @ self.Sigma_2_inverse @ mu_diff - \
+                Sigma.shape[0] + \
+                torch.log(self.Sigma_2_det/torch.det(Sigma))
+        loss = classifier_loss + self.slack * ge_penalty
+
+        if self.autoencoder > 0:
+            loss += recon_error * self.autoencoder
+
+        if self.posterior_L1 > 0:
+            r_labeled = torch.mean(torch.abs(score[Y>0]))
+            r_unlabeled = torch.mean(torch.abs(score[Y==0]))
+            r = self.posterior_L1*(r_labeled*self.labeled_fraction + r_unlabeled*(1-self.labeled_fraction))
+            loss = loss + r
+
+        loss.backward()
+
+        precision = (probs[Y>0, 1:].sum()/probs[:,1:].sum()).item()
+        tpr = probs[Y>0, 1:].mean().item()
+        fpr = probs[Y == 0, 1:].mean().item()
+
+        if self.l2 > 0:
+            r = sum(torch.sum(w**2) for w in self.model.features.parameters())
+            r = r + sum(torch.sum(w**2) for w in self.model.classifier.parameters())
+            r = 0.5*self.l2*r
+            r.backward()
+
+        self.optim.step()
+        self.optim.zero_grad()
+
+        if self.autoencoder > 0:
+            return classifier_loss.item(), ge_penalty.item(), recon_error.item(), precision, tpr, fpr
+        
+        return classifier_loss.item(), ge_penalty.item(), precision, tpr, fpr
diff --git a/topaz/training.py b/topaz/training.py
index d2cfdf2..dfacaa3 100644
--- a/topaz/training.py
+++ b/topaz/training.py
@@ -359,9 +359,9 @@ def make_training_step_method(classifier, num_positive_regions, positive_fractio
     # but during training, we iterate over unlabeled data with labeled positives removed
     # therefore, we expected the fraction of positives in the unlabeled data
     # to be pi - fraction of labeled positives
-    # if we are using the 'GE-KL' or 'GE-binomial' loss functions
+    # if we are using the 'GE-KL' or 'GE-binomial' or 'GE-multinomial' loss functions
     p_observed = positive_fraction
-    if pi <= p_observed and method in ['GE-KL', 'GE-binomial']:
+    if pi <= p_observed and method in ['GE-KL', 'GE-binomial', 'GE-multinomial']:
         # if pi <= p_observed, then we think the unlabeled data is all negatives
         # report this to the user and switch method to 'PN' if it isn't already
         print(f'WARNING: pi={pi} but the observed fraction of positives is {p_observed} and method is set to {method}.', file=sys.stderr) 
@@ -369,7 +369,7 @@ def make_training_step_method(classifier, num_positive_regions, positive_fractio
         print(f'WARNING: if you meant to use {method}, please set pi > {p_observed}.', file=sys.stderr)
         pi = p_observed
         method = 'PN'
-    elif method in ['GE-KL', 'GE-binomial']:
+    elif method in ['GE-KL', 'GE-binomial', 'GE-multinomial']:
         pi = pi - p_observed
 
     split = 'pn'
@@ -394,6 +394,13 @@ def make_training_step_method(classifier, num_positive_regions, positive_fractio
         optim = optim(classifier.parameters(), lr=lr)
         trainer = methods.PU(classifier, optim, criteria, pi, l2=l2, autoencoder=autoencoder)
 
+    elif method == 'GE-multinomial':
+        if slack < 0:
+            slack = 1
+        optim = optim(classifier.parameters(), lr=lr)
+        criteria = nn.NLLLoss()  # use negative log likelihood for multiple classes
+        trainer = methods.GE_multinomial(classifier, optim, criteria, pi, l2=l2, slack=slack, autoencoder=autoencoder)
+
     else:
         raise Exception('Invalid method: ' + method)
 
diff --git a/tutorial/01_quick_start_guide.ipynb b/tutorial/01_quick_start_guide.ipynb
index f1f644c..5402df7 100644
--- a/tutorial/01_quick_start_guide.ipynb
+++ b/tutorial/01_quick_start_guide.ipynb
@@ -1,27 +1,5 @@
 {
  "cells": [
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Quick start guide\n",
-    "\n",
-    "This notebook details the minimal steps needed to train a particle detection model and then use that model to extract predicted particle coordinates with Topaz. For a more detailed walkthrough with visualization of outputs from the different steps, please see: https://github.com/tbepler/topaz/blob/master/tutorial/02_walkthrough.ipynb\n",
-    "\n",
-    "Topaz now includes pretrained particle picking models that can be used to skip the training step for many datasets. Even when training, initializing from these models can substantially reduce the amount of training steps required. By default, topaz train will now initialize from these models.\n",
-    "\n",
-    "__Topaz is assumed to be installed in a conda environment named \"topaz\" for purposes of running topaz commands within bash cells.__ If topaz was installed in some other way, then the \"source activate topaz\" lines will need to be removed or changed below. See https://github.com/tbepler/topaz#installation for details on installing Topaz.\n",
-    "\n",
-    "### Demo dataset\n",
-    "\n",
-    "This guide uses a demo dataset that can be downloaded [here](http://bergerlab-downloads.csail.mit.edu/topaz/topaz-tutorial-data.tar.gz) and should be unpacked directly in this (the tutorial) directory.\n",
-    "\n",
-    "```\n",
-    "wget http://bergerlab-downloads.csail.mit.edu/topaz/topaz-tutorial-data.tar.gz\n",
-    "tar -xzvf topaz-tutorial-data.tar.gz\n",
-    "```"
-   ]
-  },
   {
    "cell_type": "markdown",
    "metadata": {},
@@ -66,9 +44,98 @@
   {
    "cell_type": "code",
    "execution_count": 1,
-   "metadata": {
-    "collapsed": false
-   },
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+       "\u001b[0;31mDocstring:\u001b[0m this is main!\n",
+       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\n",
+       "\u001b[0;31mType:\u001b[0m      function"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "import sys\n",
+    "import importlib\n",
+    "import topaz\n",
+    "from topaz.main import main\n",
+    "\n",
+    "# importlib.reload(topaz.main)\n",
+    "import pdb\n",
+    "main?\n",
+    "\n",
+    "# replace /miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py - DONE\n",
+    "# figure out softlinking so that my repo changes lead to changes in the miniconda package"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
+      "\n"
+     ]
+    },
+    {
+     "name": "stdin",
+     "output_type": "stream",
+     "text": [
+      "ipdb>  s\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "--Call--\n",
+      "> \u001b[0;32m/h2/kspivakovsky/miniconda3/envs/topaz/lib/python3.12/site-packages/topaz/main.py\u001b[0m(53)\u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
+      "\u001b[0;32m     51 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0m\u001b[0;32m---> 53 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0;34m'''this is main!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0m\n"
+     ]
+    },
+    {
+     "name": "stdin",
+     "output_type": "stream",
+     "text": [
+      "ipdb>  q\n"
+     ]
+    }
+   ],
+   "source": [
+    "# %debug\n",
+    "import glob\n",
+    "input_files = glob.glob('/h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc')\n",
+    "input_files = ' '.join(input_files)\n",
+    "\n",
+    "# create command\n",
+    "cmd = f'''preprocess -s 8 -o{input_files}'''\n",
+    "# cmd = f'''preprocess -s 8 -d -1 -o /fastData/topaz_tutorial_data/EMPIAR-10025/processed/m2/ {input_files}'''\n",
+    "\n",
+    "#topaz.main.main(cmd)\n",
+    "# import pdb;breakpoint()\n",
+    "# main(cmd)\n",
+    "# topaz.main.main(cmd)\n",
+    "pdb.run(f'''topaz.main.main(cmd)''')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {},
    "outputs": [
     {
      "name": "stderr",
@@ -109,12 +176,12 @@
    ],
    "source": [
     "%%bash\n",
-    "source activate topaz\n",
+    "# source activate topaz\n",
     "\n",
     "# we'll store the processed data in data/EMPIAR-10025/processed\n",
     "# so we need to make these directories first\n",
-    "mkdir -p data/EMPIAR-10025/processed\n",
-    "mkdir -p data/EMPIAR-10025/processed/micrographs\n",
+    "# mkdir -p data/EMPIAR-10025/processed\n",
+    "# mkdir -p data/EMPIAR-10025/processed/micrographs\n",
     "\n",
     "# to run the preprocess command, we pass the input micrographs as command line arguments\n",
     "# preprocess will write the processed images to the directory specified with the -o argument\n",
@@ -122,12 +189,12 @@
     "# -d/--device X sets preprocess to use GPU with ID X\n",
     "# -t/--num-workers X sets preprocess to use X processes (preprocesses X micrographs in parallel), this and GPU device are mutually exclusive\n",
     "# -v gives verbose output to track progress\n",
-    "topaz preprocess -v -s 8 -o data/EMPIAR-10025/processed/micrographs/ data/EMPIAR-10025/rawdata/micrographs/*.mrc\n",
+    "topaz preprocess -v -s 8 -o /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/micrographs/ /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/micrographs/*.mrc\n",
     "\n",
     "# this command takes the particle coordinates matched to the original micrographs\n",
     "# and scales them by 1/8 (-s is downscaling)\n",
     "# the -x option applies upscaling instead\n",
-    "topaz convert -s 8 -o data/EMPIAR-10025/processed/particles.txt data/EMPIAR-10025/rawdata/particles.txt"
+    "topaz convert -s 8 -o /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/processed/particles.txt /h2/kspivakovsky/topaz_tutorial_data/EMPIAR-10025/rawdata/particles.txt"
    ]
   },
   {
@@ -142,9 +209,7 @@
   {
    "cell_type": "code",
    "execution_count": 2,
-   "metadata": {
-    "collapsed": false
-   },
+   "metadata": {},
    "outputs": [
     {
      "name": "stderr",
@@ -213,9 +278,7 @@
   {
    "cell_type": "code",
    "execution_count": 3,
-   "metadata": {
-    "collapsed": false
-   },
+   "metadata": {},
    "outputs": [],
    "source": [
     "%%bash\n",
@@ -244,9 +307,7 @@
   },
   {
    "cell_type": "markdown",
-   "metadata": {
-    "collapsed": false
-   },
+   "metadata": {},
    "source": [
     "# (Optional) change format of particle coordinates file"
    ]
@@ -255,7 +316,10 @@
    "cell_type": "code",
    "execution_count": 4,
    "metadata": {
-    "collapsed": true
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    }
    },
    "outputs": [],
    "source": [
@@ -296,7 +360,10 @@
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
-    "collapsed": true
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    }
    },
    "outputs": [],
    "source": []
@@ -305,9 +372,9 @@
  "metadata": {
   "anaconda-cloud": {},
   "kernelspec": {
-   "display_name": "Python [conda env:topaz]",
+   "display_name": "Python 3 (ipykernel)",
    "language": "python",
-   "name": "conda-env-topaz-py"
+   "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
@@ -319,9 +386,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.12.0"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 1
+ "nbformat_minor": 4
 }
diff --git a/tutorial/katie.ipynb b/tutorial/katie.ipynb
new file mode 100644
index 0000000..fdf34f5
--- /dev/null
+++ b/tutorial/katie.ipynb
@@ -0,0 +1,37 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Katie's testing environment"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import pandas as pd"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "%% bash \n",
+    "topaz preprocess -v -s 8 -o data/EMPIAR-10025/processed/micrographs/ data/EMPIAR-10025/rawdata/micrographs/*.mrc"
+   ]
+  }
+ ],
+ "metadata": {
+  "language_info": {
+   "name": "python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
